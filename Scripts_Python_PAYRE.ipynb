{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf1be529-6ddb-4508-9ba3-cdb5752cf5ad",
   "metadata": {},
   "source": [
    "# Scripts Python - Project : Mapping Cerebral Network Alterations in CKD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64e3b1-6f36-4a32-81a1-62ffb9c187c1",
   "metadata": {},
   "source": [
    "**Léa Payre**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2ced2-a0c6-496a-9b02-ec77d447129e",
   "metadata": {},
   "source": [
    "M2 DigiPharm - *Aix Marseille Université*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86865a50-4442-4080-92e7-256717ce569d",
   "metadata": {},
   "source": [
    "# Statistiques descriptives "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd396230-cb18-400d-9236-25b606a60176",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Caractérisation des données de captation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35512747-481d-4c34-93ea-3f17d490104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fichier = \"CTRL_DTPA.xlsx\"  # à modifier par groupe x traceur\n",
    "\n",
    "df = pd.read_excel(fichier) \n",
    "\n",
    "regions = df.columns[1:] # première colonne = Sujet_ID et les suivantes = régions\n",
    "\n",
    "# fonction statistiques\n",
    "def stats_intra_sujet(valeurs):\n",
    "    return pd.Series({\n",
    "        \"Moyenne\": np.mean(valeurs),\n",
    "        \"Médiane\": np.median(valeurs),\n",
    "        \"Ecart_type\": np.std(valeurs, ddof=1),\n",
    "        \"Min\": np.min(valeurs),\n",
    "        \"Max\": np.max(valeurs),\n",
    "        \"IQR\": np.percentile(valeurs, 75) - np.percentile(valeurs, 25)\n",
    "    })\n",
    "\n",
    "# appliquer la fonction sur chaque individu\n",
    "stats_individus = df.apply(\n",
    "    lambda row: stats_intra_sujet(row[regions].values.astype(float)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ajouter ID sujet\n",
    "stats_individus.insert(0, \"Sujet_ID\", df[\"Sujet_ID\"])\n",
    "\n",
    "# exportation \n",
    "nom_sortie = fichier.replace(\".xlsx\", \"_stats_individus.xlsx\")\n",
    "stats_individus.to_excel(nom_sortie, index=False)\n",
    "\n",
    "print(f\" Statistiques descriptives intra-sujet sauvegardées dans {nom_sortie}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f95d23b-6b37-43d4-aece-c45d2ed472a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Analyses inter-sujets par région cérébrale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd367d-8446-4af0-99ef-25f8ab903c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fichier = \"CTRL_DTPA.xlsx\" \n",
    "\n",
    "df = pd.read_excel(fichier)\n",
    "\n",
    "regions = df.columns[1:]\n",
    "\n",
    "df[regions] = df[regions].apply(pd.to_numeric, errors=\"coerce\")\n",
    " \n",
    "def iqr(s):  # IQR = Q3 - Q1\n",
    "    return np.percentile(s.dropna(), 75) - np.percentile(s.dropna(), 25)\n",
    "\n",
    "def cv(s):   # coefficient de variation\n",
    "    m = s.mean()\n",
    "    return np.nan if m == 0 else (s.std(ddof=1) / m) * 100\n",
    "\n",
    "#  statistiques par groupe/region \n",
    "stats_grp = pd.DataFrame({\n",
    "    \"N\":        df[regions].count(),               # nb valeurs non-NaN\n",
    "    \"Nb_Na\":    df[regions].isna().sum(),          # nb NaN\n",
    "    \"Moyenne\":  df[regions].mean(),\n",
    "    \"Médiane\":  df[regions].median(),\n",
    "    \"Ecart_type\": df[regions].std(ddof=1),\n",
    "    \"Min\":      df[regions].min(),\n",
    "    \"Max\":      df[regions].max(),\n",
    "    \"IQR\":      df[regions].apply(iqr),\n",
    "    \"Skewness\": df[regions].skew(),                # asymétrie\n",
    "    \"CV_%\":     df[regions].apply(cv)              # coefficient de variation en %\n",
    "})\n",
    "\n",
    "# régions en colonne \n",
    "stats_grp = stats_grp.reset_index().rename(columns={\"index\": \"Region\"})\n",
    "\n",
    "nom_sortie = fichier.replace(\".xlsx\", \"_stats_groupes.xlsx\")\n",
    "stats_grp.to_excel(nom_sortie, index=False)\n",
    "\n",
    "print(f\" Statistiques descriptives par groupe sauvegardées dans {nom_sortie}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97df7d7-7830-42c9-919a-e0624d543644",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Évaluation de la normalité des distribution (Shapiro-Wilk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d16ea-5fa5-40fb-9a20-0b86140a73f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fichier_excel = \"CTRL_DTPA_Mbq.xlsx\" \n",
    "\n",
    "noms_roi = [\n",
    "    \"Olfactory_Bulb\", \"WM_Cerebellum\", \"Ventricle_III_IV\",\n",
    "    \"Ventricle_Lateral\", \"Brain_Stem\", \"WM_Ctx\",\n",
    "    \"Ctx_Parietal\", \"Ctx_Occipital\", \n",
    "    \"tx_Temporal_L\", \"Ctx_Frontal\", \n",
    "    \"Ctx_Cingulate\", \"Cerebellum\", \n",
    "    \"Amygdala\", \"Hippocampus\",\n",
    "    \"Thalamus_Hypothalamus\", \"Caudate_Putamen\",\n",
    "]\n",
    "\n",
    "# si ligne 1 = en-tête avec noms des sujets\n",
    "df = pd.read_excel(fichier_excel, header=None) \n",
    "\n",
    "# vérification du fichier\n",
    "nb_lignes_data = df.shape[0]\n",
    "print(f\"  Nombre de lignes dans le fichier : {nb_lignes_data}\")\n",
    "print(f\"  Nombre de ROI attendues : {len(noms_roi)}\")\n",
    "\n",
    "# shapiro-wilk pour chaque ROI\n",
    "print(f\"TEST DE SHAPIRO-WILK - {fichier_excel}\")\n",
    "resultats = []\n",
    "\n",
    "for idx in range(min(len(noms_roi), nb_lignes_data)):\n",
    "    nom_roi = noms_roi[idx]\n",
    "    \n",
    "    # extraction des valeurs de la ligne idx : toutes les colonnes = tous les sujets\n",
    "    valeurs = df.iloc[idx, :].values\n",
    "    \n",
    "    # convertion en numerique et suppression des valeurs manquantes\n",
    "    valeurs_numeriques = pd.to_numeric(valeurs, errors='coerce')\n",
    "    valeurs_propres = valeurs_numeriques[~np.isnan(valeurs_numeriques)]\n",
    "    \n",
    "    n = len(valeurs_propres)\n",
    "    \n",
    "    # test de Shapiro-Wilk\n",
    "    if n >= 3:\n",
    "        statistic, p_value = stats.shapiro(valeurs_propres)\n",
    "        \n",
    "        # statistiques descriptives\n",
    "        moyenne = np.mean(valeurs_propres)\n",
    "        mediane = np.median(valeurs_propres)\n",
    "        ecart_type = np.std(valeurs_propres, ddof=1)\n",
    "        q1 = np.percentile(valeurs_propres, 25)\n",
    "        q3 = np.percentile(valeurs_propres, 75)\n",
    "        skewness = stats.skew(valeurs_propres)\n",
    "        kurtosis = stats.kurtosis(valeurs_propres, fisher=True)\n",
    "        \n",
    "        # decision normalité\n",
    "        normale = \" Normale\" if p_value > 0.05 else \" Non-normale\"\n",
    "        \n",
    "        # stockage\n",
    "        resultats.append({\n",
    "            'ROI': nom_roi,\n",
    "            'n': n,\n",
    "            'Moyenne': moyenne,\n",
    "            'Mediane': mediane,\n",
    "            'Ecart-type': ecart_type,\n",
    "            'Q1': q1,\n",
    "            'Q3': q3,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis,\n",
    "            'W_statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'Normalite': normale\n",
    "        })\n",
    "        \n",
    "        # affichage\n",
    "        print(f\"{idx+1:2d}. {nom_roi:<30s} | n={n:2d} | W={statistic:.4f} | p={p_value:.5f} | {normale}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"{idx+1:2d}. {nom_roi:<30s} | n={n:2d} |   Effectif insuffisant\")\n",
    "        resultats.append({\n",
    "            'ROI': nom_roi,\n",
    "            'n': n,\n",
    "            'Moyenne': np.nan,\n",
    "            'Mediane': np.nan,\n",
    "            'Ecart-type': np.nan,\n",
    "            'Q1': np.nan,\n",
    "            'Q3': np.nan,\n",
    "            'Skewness': np.nan,\n",
    "            'Kurtosis': np.nan,\n",
    "            'W_statistic': np.nan,\n",
    "            'p_value': np.nan,\n",
    "            'Normalite': 'N/A'\n",
    "        })\n",
    "\n",
    "# résumé\n",
    "print(\"\\n\" + \"=\"*110)\n",
    "print(\" RÉSUMÉ\")\n",
    "print(\"=\"*110)\n",
    "\n",
    "df_resultats = pd.DataFrame(resultats)\n",
    "\n",
    "# comptage des ROI normales\n",
    "nb_normales = df_resultats[df_resultats['p_value'] > 0.05].shape[0]\n",
    "nb_totales = len(df_resultats)\n",
    "pct_normales = (nb_normales / nb_totales * 100) if nb_totales > 0 else 0\n",
    "\n",
    "print(f\" ROI normales (p > 0.05)     : {nb_normales}/{nb_totales} ({pct_normales:.1f}%)\")\n",
    "print(f\" ROI non-normales (p ≤ 0.05) : {nb_totales - nb_normales}/{nb_totales}\")\n",
    "\n",
    "# statistiques sur Skewness et Kurtosis\n",
    "skew_anormal = df_resultats[(df_resultats['Skewness'].abs() > 1)].shape[0]\n",
    "kurt_anormal = df_resultats[(df_resultats['Kurtosis'].abs() > 2)].shape[0]\n",
    "\n",
    "print(f\"\\n  ROI avec |Skewness| > 1    : {skew_anormal}/{nb_totales}\")\n",
    "print(f\"  ROI avec |Kurtosis| > 2    : {kurt_anormal}/{nb_totales}\")\n",
    "\n",
    "# export en excel\n",
    "nom_sortie = fichier_excel.replace('.xlsx', '_resultats_shapiro.xlsx')\n",
    "df_resultats.to_excel(nom_sortie, index=False)\n",
    "print(f\"\\n Résultats exportés dans : {nom_sortie}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*110)\n",
    "print(\" TABLEAU DÉTAILLÉ DES RÉSULTATS DU GROUPE CONTRÔLE, MARQUEUR 99mTc-DTPA\")\n",
    "print(\"=\"*110)\n",
    "\n",
    "# affichage formaté\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(df_resultats[['ROI', 'n', 'W_statistic', 'p_value', 'Skewness', 'Kurtosis', 'Normalite']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2425dfbc-f6bd-4393-bc4b-aa46ceafe22f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Visualisations exploratoires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab90361-464d-4754-96af-0255631f2d21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fbd654-7f61-4199-b92b-40b71200e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fichiers_dtpa = [\n",
    "    \"CTRL_DTPA_Mbq.xlsx\",\n",
    "    \"ARD_DTPA_Mbq.xlsx\",\n",
    "]\n",
    "# importation et fusion\n",
    "df_all = []\n",
    "for f in fichiers_dtpa:\n",
    "    df = pd.read_excel(f)\n",
    "    df = df.melt(id_vars=[\"Sujet_ID\"], var_name=\"Region\", value_name=\"Valeur\")\n",
    "    \n",
    "    # choix du groupe\n",
    "    if \"CTRL\" in f:\n",
    "        df[\"Groups\"] = \"CTRL\"\n",
    "    else:\n",
    "        df[\"Groups\"] = \"ARD\"\n",
    "    \n",
    "    df_all.append(df)\n",
    "\n",
    "df_all = pd.concat(df_all, ignore_index=True)\n",
    "\n",
    "# sanitation\n",
    "df_all[\"Valeur\"] = (\n",
    "    df_all[\"Valeur\"]\n",
    "      .astype(str)\n",
    "      .str.replace(\",\", \".\", regex=False)\n",
    "      .str.strip()\n",
    ")\n",
    "df_all[\"Valeur\"] = pd.to_numeric(df_all[\"Valeur\"], errors=\"coerce\")\n",
    "print(\"NaN restants :\", df_all[\"Valeur\"].isna().sum())\n",
    "\n",
    "# données (par traceur) \n",
    "data_dtpa = df_all \n",
    "\n",
    "# palette de couleur\n",
    "palette = {\n",
    "    \"CTRL\": \"#2E86AB\",\n",
    "    \"ARD\": \"#E63946\"\n",
    "}\n",
    "\n",
    "# boxplot\n",
    "plt.figure(figsize=(24, 10))\n",
    "sns.boxplot(data=data_dtpa, x=\"Region\", y=\"Valeur\", hue=\"Groups\", \n",
    "            showfliers=False, palette=palette, linewidth=2.5, width=0.65)\n",
    "for spine in ['bottom', 'left']:\n",
    "    ax.spines[spine].set_color('#555555')  \n",
    "    ax.spines[spine].set_linewidth(1.2)\n",
    "\n",
    "for spine in ['top', 'right']:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "\n",
    "sns.stripplot(data=data_dtpa, x=\"Region\", y=\"Valeur\", hue=\"Groups\",\n",
    "              dodge=True, alpha=0.5, color=\"black\", size=4)\n",
    "\n",
    "# limite de l'axe Y \n",
    "plt.ylim(0, 4.5e-6)  # De 0 à 3×10⁻⁵\n",
    "\n",
    "# labels \n",
    "plt.xticks(rotation=28, ha='right', fontsize=24, fontweight='bold')\n",
    "plt.yticks(ha='right', fontsize=20, fontweight='bold')\n",
    "\n",
    "# coloration des labels par groupe anatomique\n",
    "ax = plt.gca()\n",
    "\n",
    "# couleurs par groupe anatomique\n",
    "color_map = {\n",
    "    # ventricules / substance blanche\n",
    "    'Ventricle_III_LV': '#555F64',\n",
    "    'Ventricle_Lateral': '#555F64',\n",
    "    'WM_Ctx': '#555F64',\n",
    "    # tronc cerebrale\n",
    "    'Brain_Stem': '#68633B',\n",
    "    # structures cérébelleuses\n",
    "    'Cerebellum': '#4A3C6E',\n",
    "    'WM_Cerebellum': '#4A3C6E',\n",
    "    # corticale\n",
    "    'Ctx_Parietal': '#253D57',\n",
    "    'Ctx_Occipital': '#24425C',\n",
    "    'Ctx_Frontal': '#24425C',\n",
    "    'Ctx_Cingulate': '#24425C',\n",
    "    'tx_Temporal': '#24425C',\n",
    "    # sous corticales \n",
    "    'Amygdala': '#1F4035',\n",
    "    'Hippocampus': '#204035',\n",
    "    'Caudate_Putamen': '#204035',\n",
    "    'Thalamus_Hyppothalamus': \"#204035\",\n",
    "    'Olfactory_Bulb': '#204035'\n",
    "}\n",
    "\n",
    "# appliquer les couleurs aux labels\n",
    "for i, ticklabel in enumerate(ax.get_xticklabels()):\n",
    "    roi_name = ticklabel.get_text()\n",
    "    if roi_name in color_map:\n",
    "        ticklabel.set_color(color_map[roi_name])\n",
    "        ticklabel.set_fontweight('bold')\n",
    "        \n",
    "\n",
    "plt.xlabel(\"Brain Regions\\n\", fontsize=26, fontweight='bold')\n",
    "plt.ylabel(\"Uptake (ID/g ×10⁻⁶)\\n\", fontsize=22, fontweight='bold')\n",
    "plt.title(\"\\nBlood–Brain Barrier Permeability (⁹⁹ᵐTc-DTPA) : Control vs ARD Groups\\n\",\n",
    "          fontsize=29, fontweight='bold', pad=6)\n",
    "\n",
    "# légende\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(handles[:2], labels[:2], \n",
    "           title='Groups', title_fontsize=18, fontsize=15,\n",
    "           loc='upper right', frameon=True, shadow=True)\n",
    "for i in range(len(ax.get_xticks()) - 1):\n",
    "    ax.axvline(x=i + 0.5, color=\"#BBBBBB\", linestyle='-', linewidth=0.6, alpha=0.4)\n",
    "    \n",
    "# dictionnaire de noms affichés en anglais\n",
    "custom_labels = {\n",
    "    \"Ctx_Frontal\": \"Frontal Cortex\",\n",
    "    \"Ctx_Parietal\": \"Parietal Cortex\",\n",
    "    \"tx_Temporal\": \"Temporal Cortex\",\n",
    "    \"Ctx_Occipital\": \"Occipital Cortex\",\n",
    "    \"Ctx_Cingulate\": \"Cingulate Cortex\",\n",
    "    \"Olfactory_Bulb\": \"Olfactory Bulb\",\n",
    "    \"Amygdala\": \"Amygdala\",\n",
    "    \"Hippocampus\": \"Hippocampus\",\n",
    "    \"Thalamus_Hyppothalamus\": \"Thalamus – Hypothalamus\",\n",
    "    \"Caudate_Putamen\": \"Caudate – Putamen\",\n",
    "    \"Cerebellum\": \"Cerebellum\",\n",
    "    \"WM_Cerebellum\": \"Cerebellar White Matter\",\n",
    "    \"Brain_Stem\": \"Brainstem\",\n",
    "    \"WM_Ctx\": \"Subcortical White Matter\",\n",
    "    \"Ventricle_Lateral\": \"Lateral Ventricles\",\n",
    "    \"Ventricle_III_IV\": \"III–IV Ventricles\"\n",
    "}\n",
    "\n",
    "# remplacer les labels affichés sur l'axe X\n",
    "ax.set_xticklabels([\n",
    "    custom_labels.get(label.get_text(), label.get_text())\n",
    "    for label in ax.get_xticklabels()\n",
    "])\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(handles[:2], labels[:2],  # Garder seulement CTRL et ARD (les 2 premiers)\n",
    "           title='Groups', loc='lower right', bbox_to_anchor=(1.0, 1.02), \n",
    "           frameon=True, ncol=2, fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# export en png et pdf\n",
    "plt.savefig('DTPA_boxplot_CTRL_vs_ARD.png', \n",
    "            dpi=600, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.savefig('DTPA_boxplot_CTRL_vs_ARD.pdf', \n",
    "            format='pdf', bbox_inches='tight',\n",
    "            facecolor='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ca3df9-5af2-4888-90c8-89aa7c96ae9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d6e2a-a073-4da5-bd5c-beac53fb611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np                         \n",
    "from matplotlib import patheffects as pe   \n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Helvetica',\n",
    "    'pdf.fonttype': 42,  \n",
    "})\n",
    "\n",
    "fichiers = [\"CTRL_DTPA_Mbq.xlsx\", \"ARD_DTPA_Mbq.xlsx\"]\n",
    "infos = [(f, \"CTRL\" if \"CTRL\" in f else \"ARD\") for f in fichiers]\n",
    "\n",
    "df_all = []\n",
    "for f, grp in infos:\n",
    "    df = pd.read_excel(f)\n",
    "    df = df.melt(id_vars=[\"Sujet_ID\"], var_name=\"Region\", value_name=\"Valeur\")\n",
    "    df[\"Groups\"] = grp\n",
    "    df_all.append(df)\n",
    "\n",
    "df_all = pd.concat(df_all, ignore_index=True)\n",
    "df_all[\"Valeur\"] = (\n",
    "    df_all[\"Valeur\"].astype(str).str.replace(\",\", \".\", regex=False).str.strip()\n",
    ")\n",
    "df_all[\"Valeur\"] = pd.to_numeric(df_all[\"Valeur\"], errors=\"coerce\")\n",
    "\n",
    "moyennes = (\n",
    "    df_all.groupby([\"Groups\", \"Region\"])[\"Valeur\"].mean().reset_index()\n",
    ")\n",
    "\n",
    "pivot = moyennes.pivot(index=\"Groups\", columns=\"Region\", values=\"Valeur\").reindex([\"CTRL\", \"ARD\"])\n",
    "vals = pivot.values.flatten()\n",
    "vmin = pd.Series(vals).quantile(0.02)\n",
    "vmax = pd.Series(vals).quantile(0.98)\n",
    "\n",
    "ROI_ORDER = [\n",
    "    \"Olfactory_Bulb\", \"WM_Cerebellum\", \"Ventricle_III_IV\", \"Ventricle_Lateral\",\n",
    "    \"Brain_Stem\", \"WM_Ctx\", \"Ctx_Parietal\", \"Ctx_Occipital\", \"tx_Temporal\",\n",
    "    \"Ctx_Frontal\", \"Ctx_Cingulate\", \"Cerebellum\", \"Amygdala\", \"Hippocampus\",\n",
    "    \"Thalamus_Hyppothalamus\", \"Caudate_Putamen\",\n",
    "]\n",
    "cols = [c for c in ROI_ORDER if c in pivot.columns] + [c for c in pivot.columns if c not in ROI_ORDER]\n",
    "pivot = pivot.loc[:, cols]\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"Olfactory_Bulb\": \"Olfactory Bulb\",\n",
    "    \"WM_Cerebellum\": \"Cerebellar White Matter\",\n",
    "    \"Ventricle_III_IV\": \"III–IV Ventricles\",\n",
    "    \"Ventricle_Lateral\": \"Lateral Ventricles\",\n",
    "    \"Brain_Stem\": \"Brainstem\",\n",
    "    \"WM_Ctx\": \"Subcortical White Matter\",\n",
    "    \"Ctx_Parietal\": \"Parietal Cortex\",\n",
    "    \"Ctx_Occipital\": \"Occipital Cortex\",\n",
    "    \"tx_Temporal\": \"Temporal Cortex\",\n",
    "    \"Ctx_Frontal\": \"Frontal Cortex\",\n",
    "    \"Ctx_Cingulate\": \"Cingulate Cortex\",\n",
    "    \"Cerebellum\": \"Cerebellum\",\n",
    "    \"Amygdala\": \"Amygdala\",\n",
    "    \"Hippocampus\": \"Hippocampus\",\n",
    "    \"Thalamus_Hyppothalamus\": \"Thalamus – Hypothalamus\",\n",
    "    \"Caudate_Putamen\": \"Caudate – Putamen\",\n",
    "}\n",
    "pivot_disp = pivot.copy()\n",
    "pivot_disp.columns = [LABEL_MAP.get(c, c) for c in pivot.columns]\n",
    "\n",
    "#  heatmap \n",
    "plt.figure(figsize=(20, 5))\n",
    "ax = sns.heatmap(\n",
    "    pivot_disp, cmap=\"plasma\", vmin=vmin, vmax=vmax,\n",
    "    annot=False, cbar_kws={'label': 'Mean tracer uptake (ID/g)'}\n",
    ")\n",
    "\n",
    "# colorbar \n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.set_ylabel(\"\\nMean tracer uptake (ID/g)\", fontsize=14, fontweight='semibold')\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "color_map = {\n",
    "    # ventricules / substance blanche\n",
    "    'III–IV Ventricles': '#555F64',\n",
    "    'Lateral Ventricles': '#555F64',\n",
    "    'Subcortical White Matter': '#555F64',\n",
    "    # tronc cérébral\n",
    "    'Brainstem': '#68633B',\n",
    "    # structures cerebelleuses\n",
    "    'Cerebellum': '#4A3C6E',\n",
    "    'Cerebellar White Matter': '#4A3C6E',\n",
    "    # corticales\n",
    "    'Parietal Cortex': '#253D57',\n",
    "    'Occipital Cortex': '#24425C',\n",
    "    'Frontal Cortex': '#24425C',\n",
    "    'Cingulate Cortex': '#24425C',\n",
    "    'Temporal Cortex': '#24425C',\n",
    "    # sous corticales\n",
    "    'Amygdala': '#204035',\n",
    "    'Hippocampus': '#204035',\n",
    "    'Caudate – Putamen': '#204035',\n",
    "    'Thalamus – Hypothalamus': \"#204035\",\n",
    "    'Olfactory Bulb': '#204035'\n",
    "}\n",
    "BASE_LABEL_COLOR = \"#2B3A42\"  \n",
    "\n",
    "# centrer les ticks au milieu des cellules\n",
    "ax.set_xticks(np.arange(pivot_disp.shape[1]) + 0.5)\n",
    "\n",
    "# labels avec ancrage correct\n",
    "ax.set_xticklabels(\n",
    "    list(pivot_disp.columns),\n",
    "    rotation=28, ha=\"right\", rotation_mode=\"anchor\",\n",
    "    fontsize=18\n",
    ")\n",
    "\n",
    "#  gras + halo blanc + couleur par catégorie\n",
    "for lbl in ax.get_xticklabels():\n",
    "    txt = lbl.get_text()\n",
    "    lbl.set_fontweight('bold')\n",
    "    lbl.set_color(color_map.get(txt, BASE_LABEL_COLOR))\n",
    "    lbl.set_path_effects([pe.withStroke(linewidth=2.0, foreground=\"white\")])\n",
    "\n",
    "for lbl in ax.get_yticklabels():\n",
    "    lbl.set_fontweight('bold')\n",
    "    lbl.set_color(BASE_LABEL_COLOR)\n",
    "    lbl.set_path_effects([pe.withStroke(linewidth=2.0, foreground=\"white\")])\n",
    "\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, ha=\"right\", fontsize=20, fontweight=\"bold\")\n",
    "\n",
    "plt.title(\"Heatmap of Blood–Brain Barrier Permeability (⁹⁹ᵐTc-DTPA) : Control vs ARD Groups\\n\",\n",
    "          fontsize=22, pad=6, weight='bold')\n",
    "plt.xlabel(\"Brain Regions\", fontsize=20, weight='bold')\n",
    "plt.ylabel(\" Groups\\n\", fontsize=18, weight='bold')\n",
    "\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('DTPA_heatmap_CTRL_vs_ARD.png', dpi=600, bbox_inches='tight', facecolor='white')\n",
    "plt.savefig('DTPA_heatmap_CTRL_vs_ARD.pdf', format='pdf', bbox_inches='tight', facecolor='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d7305-1712-4276-a52e-f7b943f90615",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Analyse statistique intergroupe (Mann-Whitney)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0747fb-cd3f-476f-8938-2f2498d23ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu, shapiro\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def load_data(ctrl_file, ard_file, sheet_name=0):\n",
    "    \"\"\"\n",
    "    charge les données CTRL et ARD depuis des fichiers Excel\n",
    "    \n",
    "    parameters:\n",
    "    ctrl_file : str\n",
    "        chemin vers le fichier Excel des contrôles\n",
    "    ard_file : str\n",
    "        chemin vers le fichier Excel des ARD\n",
    "    sheet_name : str or int\n",
    "        nom ou index de la feuille à charger\n",
    "    \n",
    "    Returns:\n",
    "\n",
    "    df_ctrl, df_ard : pandas.DataFrame\n",
    "        DataFrames contenant les données\n",
    "    \"\"\"\n",
    "    df_ctrl = pd.read_excel(ctrl_file, sheet_name=sheet_name)\n",
    "    df_ard = pd.read_excel(ard_file, sheet_name=sheet_name)\n",
    "    \n",
    "    print(f\" Données CTRL chargées: {len(df_ctrl)} observations\")\n",
    "    print(f\"Données ARD chargées: {len(df_ard)} observations\")\n",
    "    \n",
    "    return df_ctrl, df_ard\n",
    "\n",
    "\n",
    "def get_roi_columns(df):\n",
    "    \"\"\"\n",
    "    extrait les colonnes correspondant aux ROIs (exclut Sujet_ID)\n",
    "    \n",
    "    parameters:\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame contenant les données\n",
    "    \n",
    "    returns:\n",
    "    list : Liste des noms de colonnes des ROIs\n",
    "    \"\"\"\n",
    "    # on exclu la colonne Sujet_ID\n",
    "    roi_cols = [col for col in df.columns if col not in ['Sujet_ID', 'Subject_ID', 'ID']]\n",
    "    return roi_cols\n",
    "\n",
    "def test_normality(data, roi_name):\n",
    "    \"\"\"\n",
    "    Teste la normalité des données avec le test de Shapiro-Wilk\n",
    "    \n",
    "    parameters:\n",
    "    data : array-like\n",
    "        Données à tester\n",
    "    roi_name : str\n",
    "        Nom de la ROI (pour affichage)\n",
    "    \n",
    "    Returns:\n",
    "    statistic, p_value : float\n",
    "        statistique et p-value du test de Shapiro\n",
    "    \"\"\"\n",
    "    if len(data) < 3:\n",
    "        return None, None\n",
    "    \n",
    "    stat, p = shapiro(data)\n",
    "    return stat, p\n",
    "\n",
    "\n",
    "def mann_whitney_comparison(df_ctrl, df_ard, roi_cols, alpha=0.05, apply_fdr=True):\n",
    "    \"\"\"\n",
    "    Effectue les tests de Mann-Whitney pour chaque ROI avec correction FDR\n",
    "    \n",
    "    parameters:\n",
    "    df_ctrl : pandas.DataFrame\n",
    "        Données du groupe contrôle\n",
    "    df_ard : pandas.DataFrame\n",
    "        Données du groupe ARD\n",
    "    roi_cols : list\n",
    "        Liste des noms de colonnes des ROIs\n",
    "    alpha : float\n",
    "        Seuil de significativité (défaut: 0.05)\n",
    "    apply_fdr : bool\n",
    "        Appliquer la correction FDR de Benjamini-Hochberg (défaut: True)\n",
    "    \n",
    "    Returns:\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame avec les résultats statistiques pour chaque ROI\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSE STATISTIQUE - TEST DE MANN-WHITNEY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for roi in roi_cols:\n",
    "        # extraction des données pour cette ROI\n",
    "        ctrl_data = df_ctrl[roi].dropna().values\n",
    "        ard_data = df_ard[roi].dropna().values\n",
    "        \n",
    "        # statistiques descriptives\n",
    "        ctrl_median = np.median(ctrl_data)\n",
    "        ard_median = np.median(ard_data)\n",
    "        ctrl_mean = np.mean(ctrl_data)\n",
    "        ard_mean = np.mean(ard_data)\n",
    "        \n",
    "        # tests de normalité\n",
    "        ctrl_shapiro_stat, ctrl_shapiro_p = test_normality(ctrl_data, f\"{roi}_CTRL\")\n",
    "        ard_shapiro_stat, ard_shapiro_p = test_normality(ard_data, f\"{roi}_ARD\")\n",
    "        \n",
    "        # test de Mann-Whitney\n",
    "        statistic, p_value = mannwhitneyu(ctrl_data, ard_data, alternative='two-sided')\n",
    "        \n",
    "        # calcul de l'effect size (r = Z / sqrt(N))\n",
    "        n1, n2 = len(ctrl_data), len(ard_data)\n",
    "        N = n1 + n2\n",
    "        # approximation de Z à partir de U\n",
    "        U = statistic\n",
    "        mu_U = n1 * n2 / 2\n",
    "        sigma_U = np.sqrt(n1 * n2 * (N + 1) / 12)\n",
    "        Z = (U - mu_U) / sigma_U\n",
    "        effect_size = abs(Z) / np.sqrt(N)\n",
    "        \n",
    "        results.append({\n",
    "            'ROI': roi,\n",
    "            'n_CTRL': n1,\n",
    "            'n_ARD': n2,\n",
    "            'Median_CTRL': ctrl_median,\n",
    "            'Median_ARD': ard_median,\n",
    "            'Mean_CTRL': ctrl_mean,\n",
    "            'Mean_ARD': ard_mean,\n",
    "            'Shapiro_p_CTRL': ctrl_shapiro_p,\n",
    "            'Shapiro_p_ARD': ard_shapiro_p,\n",
    "            'U_statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'effect_size_r': effect_size\n",
    "        })\n",
    "    \n",
    "    # creation du DataFrame des résultats\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # correction FDR \n",
    "    if apply_fdr:\n",
    "\n",
    "        reject, pvals_corrected, alphacSidak, alphacBonf = multipletests(\n",
    "            results_df['p_value'], \n",
    "            alpha=alpha, \n",
    "            method='fdr_bh'\n",
    "        )\n",
    "        results_df['p_value_FDR'] = pvals_corrected\n",
    "        results_df['Significant_FDR'] = reject\n",
    "    else:\n",
    "        results_df['Significant'] = results_df['p_value'] < alpha\n",
    "    \n",
    "    # symboles de significativité\n",
    "    def get_significance_symbol(p, fdr=apply_fdr):\n",
    "        p_val = p if not fdr else p\n",
    "        if p_val < 0.001:\n",
    "            return '***'\n",
    "        elif p_val < 0.01:\n",
    "            return '**'\n",
    "        elif p_val < 0.05:\n",
    "            return '*'\n",
    "        else:\n",
    "            return 'ns'\n",
    "    \n",
    "    if apply_fdr:\n",
    "        results_df['Significance'] = results_df['p_value_FDR'].apply(\n",
    "            lambda x: get_significance_symbol(x, fdr=True)\n",
    "        )\n",
    "    else:\n",
    "        results_df['Significance'] = results_df['p_value'].apply(\n",
    "            lambda x: get_significance_symbol(x, fdr=False)\n",
    "        )\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def print_summary(results_df, apply_fdr=True):\n",
    "    \"\"\"\n",
    "    affiche un résumé des résultats statistiques\n",
    "    \n",
    "    parameters:\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame avec les résultats\n",
    "    apply_fdr : bool\n",
    "        Si True, utilise les p-values corrigées FDR\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RÉSUMÉ DES RÉSULTATS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if apply_fdr:\n",
    "        sig_col = 'Significant_FDR'\n",
    "        p_col = 'p_value_FDR'\n",
    "        print(\"Correction FDR (Benjamini-Hochberg) appliquée\")\n",
    "    else:\n",
    "        sig_col = 'Significant'\n",
    "        p_col = 'p_value'\n",
    "        print(\"Pas de correction pour comparaisons multiples\")\n",
    "    \n",
    "    print(f\"\\nNombre total de ROIs testées: {len(results_df)}\")\n",
    "    \n",
    "    if apply_fdr:\n",
    "        n_sig = results_df[sig_col].sum()\n",
    "    else:\n",
    "        n_sig = (results_df[p_col] < 0.05).sum()\n",
    "    \n",
    "    print(f\"ROIs significatives (p < 0.05): {n_sig}\")\n",
    "    print(f\"ROIs non significatives: {len(results_df) - n_sig}\")\n",
    "    \n",
    "    # niveaux de significativité\n",
    "    n_p001 = (results_df[p_col] < 0.001).sum()\n",
    "    n_p01 = ((results_df[p_col] >= 0.001) & (results_df[p_col] < 0.01)).sum()\n",
    "    n_p05 = ((results_df[p_col] >= 0.01) & (results_df[p_col] < 0.05)).sum()\n",
    "    \n",
    "    print(f\"\\nNiveaux de significativité:\")\n",
    "    print(f\"  *** (p < 0.001): {n_p001}\")\n",
    "    print(f\"  **  (p < 0.01):  {n_p01}\")\n",
    "    print(f\"  *   (p < 0.05):  {n_p05}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"ROIs significatives:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    if apply_fdr:\n",
    "        sig_rois = results_df[results_df[sig_col]]\n",
    "    else:\n",
    "        sig_rois = results_df[results_df[p_col] < 0.05]\n",
    "    \n",
    "    if len(sig_rois) > 0:\n",
    "        for idx, row in sig_rois.iterrows():\n",
    "            direction = \"ARD > CTRL\" if row['Median_ARD'] > row['Median_CTRL'] else \"ARD < CTRL\"\n",
    "            print(f\"{row['ROI']:30s} {row['Significance']:5s} (p = {row[p_col]:.4f}) [{direction}] r = {row['effect_size_r']:.3f}\")\n",
    "    else:\n",
    "        print(\"Aucune ROI significative\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Test de normalité (Shapiro-Wilk):\")\n",
    "    print(\"-\"*80)\n",
    "    non_normal_ctrl = (results_df['Shapiro_p_CTRL'] < 0.05).sum()\n",
    "    non_normal_ard = (results_df['Shapiro_p_ARD'] < 0.05).sum()\n",
    "    print(f\"ROIs non-normales dans CTRL: {non_normal_ctrl}/{len(results_df)}\")\n",
    "    print(f\"ROIs non-normales dans ARD: {non_normal_ard}/{len(results_df)}\")\n",
    "    print(\"→ Utilisation du test de Mann-Whitney justifiée\" if (non_normal_ctrl > 0 or non_normal_ard > 0) else \"→ Les données sont normales\")\n",
    "\n",
    "# fichiers\n",
    "CTRL_FILE = 'CTRL_DTPA_Mbq.xlsx' \n",
    "ARD_FILE = 'ARD_DTPA_Mbq.xlsx'   \n",
    "\n",
    "# options d'analyse\n",
    "APPLY_FDR = True  # True = avec correction FDR, False = sans correction\n",
    "ALPHA = 0.05      # seuil de significativité alpha\n",
    "\n",
    "# sortie\n",
    "OUTPUT_FILE = 'mann_whitney_results_DTPA.xlsx' \n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ANALYSE MANN-WHITNEY - COMPARAISON CTRL vs ARD - DTPA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# charger les données\n",
    "df_ctrl, df_ard = load_data(CTRL_FILE, ARD_FILE)\n",
    "\n",
    "# identification des colonnes \n",
    "roi_cols = get_roi_columns(df_ctrl)\n",
    "print(f\"\\n {len(roi_cols)} ROIs identifiées\")\n",
    "\n",
    "# tests statistiques\n",
    "results_df = mann_whitney_comparison(\n",
    "    df_ctrl, df_ard, roi_cols, \n",
    "    alpha=ALPHA, \n",
    "    apply_fdr=APPLY_FDR\n",
    ")\n",
    "\n",
    "# résumé\n",
    "print_summary(results_df, apply_fdr=APPLY_FDR)\n",
    "\n",
    "# export \n",
    "results_df.to_excel(OUTPUT_FILE, index=False)\n",
    "print(f\"\\n Résultats sauvegardés dans: {OUTPUT_FILE}\")\n",
    "\n",
    "# tableau des résultats\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLEAU RÉCAPITULATIF\")\n",
    "print(\"=\"*80)\n",
    "display(results_df[['ROI', 'Significance', 'p_value', 'p_value_FDR', 'Median_CTRL', 'Median_ARD', 'effect_size_r']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59490a8f-5f8b-4b82-870e-c668a1e3d11d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Standardisation par z-scores robustes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6387f89e-c94a-465b-bb21-41351b4a1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fichier = \"CTRL_DTPA.xlsx\"\n",
    "\n",
    "df = pd.read_excel(fichier)\n",
    "\n",
    "# identification des colonnes de régions (hors Sujet_ID et Joue)\n",
    "regions = [col for col in df.columns if col not in ['Sujet_ID', 'Joue']]\n",
    "\n",
    "# fonction z-scores robuste (MAD)\n",
    "def robust_zscore_mad(data):\n",
    "    \"\"\"\n",
    "    calcule les z-scores robustes basés sur la médiane et le MAD.\n",
    "    \n",
    "    Args:\n",
    "        data: array ou Series de valeurs\n",
    "    \n",
    "    Returns:\n",
    "        z-scores robustes\n",
    "    \"\"\"\n",
    "    # toutes les valeurs en un seul vecteur (applatissement)\n",
    "    all_values = data.values.flatten()\n",
    "    all_values = all_values[~np.isnan(all_values)]  # NaN\n",
    "    \n",
    "    # calcul médiane du groupe (sur toutes les ROI, tous les sujets)\n",
    "    median_group = np.median(all_values)\n",
    "    \n",
    "    # calcul MAD (Median Absolute Deviation)\n",
    "    mad = np.median(np.abs(all_values - median_group))\n",
    "    \n",
    "    # facteur de normalisation pour rendre MAD comparable à sd\n",
    "    mad_normalized = mad * 1.4826\n",
    "    \n",
    "    # protection contre MAD = 0\n",
    "    if mad_normalized == 0:\n",
    "        print(\" Attention : MAD = 0, utilisation de IQR comme fallback\")\n",
    "        q75 = np.percentile(all_values, 75)\n",
    "        q25 = np.percentile(all_values, 25)\n",
    "        iqr = q75 - q25\n",
    "        if iqr == 0:\n",
    "            return data - median_group  # Retourner différences brutes\n",
    "        return (data - median_group) / iqr\n",
    "    \n",
    "    # calcul z-scores robustes\n",
    "    z_scores = (data - median_group) / mad_normalized\n",
    "    \n",
    "    return z_scores\n",
    "\n",
    "# standardisation\n",
    "df_zscores = df.copy()\n",
    "\n",
    "# application de la transformation sur toutes les régions \n",
    "df_zscores[regions] = robust_zscore_mad(df[regions])\n",
    "\n",
    "# verification\n",
    "all_z = df_zscores[regions].values.flatten()\n",
    "all_z = all_z[~np.isnan(all_z)]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VÉRIFICATION DE LA STANDARDISATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Médiane des z-scores : {np.median(all_z):.6f}  (attendu ≈ 0)\")\n",
    "print(f\"MAD des z-scores     : {np.median(np.abs(all_z - np.median(all_z))) * 1.4826:.6f}  (attendu ≈ 1)\")\n",
    "print(f\"Moyenne des z-scores : {np.mean(all_z):.6f}\")\n",
    "print(f\"Écart-type z-scores  : {np.std(all_z):.6f}\")\n",
    "print(f\"Min z-score          : {np.min(all_z):.3f}\")\n",
    "print(f\"Max z-score          : {np.max(all_z):.3f}\")\n",
    "print(f\"% |z| > 2            : {100 * np.sum(np.abs(all_z) > 2) / len(all_z):.1f}%  (attendu ≈ 5%)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# export\n",
    "nom_sortie = fichier.replace(\"_ratios.xlsx\", \"_zscores.xlsx\")\n",
    "df_zscores.to_excel(nom_sortie, index=False)\n",
    "\n",
    "print(f\" Z-scores robustes sauvegardés dans : {nom_sortie}\")\n",
    "print(f\" Dimensions : {df_zscores.shape[0]} sujets × {len(regions)} ROI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8d768-d2a3-4591-8179-1b51306a80ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Matrice de Spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81969515-571c-4c35-aeeb-90d7734ea768",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Creation de la matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91500838-79f4-4032-9500-256d8c44a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# chargement des données z-score (par sujet × région)\n",
    "df = pd.read_excel(\"CTRL_DTPA.xlsx\", index_col=0)\n",
    "\n",
    "# calcul de la matrice de corrélation \n",
    "corr_matrix, pval_matrix = spearmanr(df, axis=0)\n",
    "\n",
    "# convertion en DataFrame\n",
    "corr_df = pd.DataFrame(corr_matrix, index=df.columns, columns=df.columns)\n",
    "pval_df = pd.DataFrame(pval_matrix, index=df.columns, columns=df.columns)\n",
    "\n",
    "# export\n",
    "corr_df.to_excel(\"CTRL_DTPA_corr_spearman.xlsx\")\n",
    "pval_df.to_excel(\"CTRL_DTPA_pval_spearman.xlsx\")\n",
    "\n",
    "print(\" Matrices Spearman exportées pour CTRL_DTPA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855adfd9-17ce-4f8e-89ba-a173fbaed664",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Calcul des differences matricielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78f68b-ae2a-4be3-a7e5-589fc4174e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# importation des matrice \n",
    "ctrl = pd.read_excel(\"CTRL_DTPA_corr_spearman.xlsx\", index_col=0)\n",
    "ard = pd.read_excel(\"ARD_DTPA_corr_spearman.xlsx\", index_col=0)\n",
    "\n",
    "# matrices differentielles\n",
    "diff_ctrl_ard = ctrl - ard\n",
    "\n",
    "# export\n",
    "diff_ctrl_ard.to_excel(\"DTPA_DIFF_CTRL-ARD_spearman.xlsx\")\n",
    "\n",
    "print(\" Matrices différentielles DTPA (Spearman) exportées\")\n",
    "\n",
    "# correlation entre matrices\n",
    "def compare_matrices(mat1, mat2, label):\n",
    "    vals1 = mat1.values.flatten()\n",
    "    vals2 = mat2.values.flatten()\n",
    "    rho, p = spearmanr(vals1, vals2)\n",
    "    print(f\"{label} : Similarité Spearman entre matrices : ρ = {rho:.3f}, p = {p:.6f}\")\n",
    "\n",
    "compare_matrices(ctrl, ard, \"DTPA CTRL vs ARD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e122f1-fa13-44af-98d1-040514889444",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Matrice de Pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e45280-4d5d-4fbb-9ae3-ea945921bd28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Creation des matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bada7a5-3b00-4e2b-a235-1b01d79b301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "fichier = \"CTRL_DTPA.xlsx\" \n",
    "\n",
    "df = pd.read_excel(fichier)\n",
    "regions = df.columns[1:] \n",
    "X = df[regions].to_numpy(dtype=float)\n",
    "\n",
    "R = np.corrcoef(X, rowvar=False)\n",
    "\n",
    "n = X.shape[0]  \n",
    "dfree = n - 2\n",
    "R_clipped = np.clip(R, -0.999999, 0.999999)  \n",
    "T = R_clipped * np.sqrt(dfree / (1 - R_clipped**2))\n",
    "P = 2 * t.sf(np.abs(T), dfree) \n",
    "\n",
    "corr_file = fichier.replace(\".xlsx\", \"_corr_pearson.xlsx\")\n",
    "pval_file = fichier.replace(\".xlsx\", \"_pval_pearson.xlsx\")\n",
    "\n",
    "pd.DataFrame(R, index=regions, columns=regions).to_excel(corr_file)\n",
    "pd.DataFrame(P, index=regions, columns=regions).to_excel(pval_file)\n",
    "\n",
    "print(f\" Matrice de corrélation Pearson sauvegardée : {corr_file}\")\n",
    "print(f\" Matrice de p-values sauvegardée : {pval_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27a051-fb54-40c6-9c3b-8232a98178ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Calcul des matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407194c-dc09-40ea-921d-19492670c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "ctrl = pd.read_excel(\"CTRL_DTPA_corr_pearson.xlsx\", index_col=0)\n",
    "ard = pd.read_excel(\"ARD_DTPA_corr_pearson.xlsx\", index_col=0)\n",
    "\n",
    "diff_ctrl_ard = ctrl - ard\n",
    "\n",
    "diff_ctrl_ard.to_excel(\"DTPA_DIFF_CTRL-ARD.xlsx\")\n",
    "\n",
    "print(\" Matrices différentielles DTPA exportées\")\n",
    "\n",
    "def compare_matrices(mat1, mat2, label):\n",
    "    vals1 = mat1.values.flatten()\n",
    "    vals2 = mat2.values.flatten()\n",
    "    r, p = pearsonr(vals1, vals2)\n",
    "    print(f\"{label} : Similarité Pearson entre matrices : r = {r:.3f}, p = {p:.4f}\")\n",
    "\n",
    "compare_matrices(ctrl, ard, \"DTPA CTRL vs ARD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f4fb7-fec2-42e8-8606-54c068dc1e32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Topologie des réseaux de corrélation par groupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a79747-bd68-49d2-9783-6e6afeef55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "fichier_corr = \"CTRL_DTPA_corr_spearman.xlsx\"\n",
    "titre = \"CTRL - DTPA (Perméabilité BBB)\"\n",
    "seuil_corr = 0.70  # thresholding\n",
    "\n",
    "# parametres visuels\n",
    "figsize = (16, 16)\n",
    "node_radius = 0.035\n",
    "edge_width_scale = 6\n",
    "font_size = 9\n",
    "label_distance = 0.10\n",
    "\n",
    "# importation des matrices de corrélation\n",
    "print(\"=\"*70)\n",
    "print(\"GÉNÉRATION CONNECTOGRAMME CIRCULAIRE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Fichier : {fichier_corr}\")\n",
    "print(f\"Seuil de corrélation : |ρ| ≥ {seuil_corr}\")\n",
    "\n",
    "df_corr = pd.read_excel(fichier_corr, index_col=0)\n",
    "\n",
    "# liste des ROI\n",
    "roi_names = df_corr.index.tolist()\n",
    "\n",
    "# ordre sagittal d'affichage (gauche vers la droite)\n",
    "roi_order_sagittal = [\n",
    "    \"Ctx_Frontal\",\n",
    "    \"Olfactory_Bulb\",\n",
    "    \"Ctx_Temporal\",\n",
    "    \"Amygdala\",\n",
    "    \"Hippocampus\",\n",
    "    \"Caudate_Putamen\",\n",
    "    \"Thalamus_Hypothalamus\",\n",
    "    \"Ctx_Cingulate\",\n",
    "    \"Ctx_Parietal\",\n",
    "    \"Ctx_Occipital\",\n",
    "    \"Cerebellum\",\n",
    "    \"WM_Cerebellum\",\n",
    "    \"Brain_Stem\",\n",
    "    \"WM_Ctx\",\n",
    "    \"Ventricle_Lateral\",\n",
    "    \"Ventricle_III_IV\",\n",
    "]\n",
    "\n",
    "# on garde seulement les ROI présents\n",
    "ordered_roi = [r for r in roi_order_sagittal if r in roi_names] + \\\n",
    "              [r for r in roi_names if r not in roi_order_sagittal]\n",
    "\n",
    "# angle de départ (180° = à gauche, pour le cortex frontal) puis sens horaire\n",
    "start_angle_deg = 180.0\n",
    "angles_ordered = np.linspace(\n",
    "    np.deg2rad(start_angle_deg),\n",
    "    np.deg2rad(start_angle_deg) - 2*np.pi,\n",
    "    len(ordered_roi),\n",
    "    endpoint=False\n",
    ")\n",
    "\n",
    "# dictionnaire angle par ROI (clé = nom du ROI) \n",
    "angle_by_roi = {roi: ang for roi, ang in zip(ordered_roi, angles_ordered)}\n",
    "\n",
    "n_roi = len(roi_names)\n",
    "\n",
    "print(f\"\\n Matrice chargée : {df_corr.shape}\")\n",
    "print(f\"   Nombre de ROI : {n_roi}\")\n",
    "print(f\"   Corrélation min : {df_corr.min().min():.3f}\")\n",
    "print(f\"   Corrélation max : {df_corr.max().max():.3f}\")\n",
    "\n",
    "# extraction des aretes\n",
    "edges = []\n",
    "for i in range(n_roi):\n",
    "    for j in range(i+1, n_roi):\n",
    "        corr = df_corr.iloc[i, j]\n",
    "        if abs(corr) >= seuil_corr:\n",
    "            edges.append({\n",
    "                'source': roi_names[i],\n",
    "                'target': roi_names[j],\n",
    "                'correlation': corr,\n",
    "                'abs_corr': abs(corr)\n",
    "            })\n",
    "\n",
    "edges_df = pd.DataFrame(edges)\n",
    "n_connections = len(edges_df)\n",
    "n_possible = int(n_roi * (n_roi - 1) / 2)\n",
    "\n",
    "print(f\"\\n Connexions extraites : {n_connections}/{n_possible} ({100*n_connections/n_possible:.1f}%)\")\n",
    "\n",
    "if n_connections == 0:\n",
    "    print(f\"\\n ATTENTION : Aucune connexion au seuil {seuil_corr}\")\n",
    "    print(\"   → Réduisez le seuil (ex: 0.6 ou 0.5)\")\n",
    "    exit()\n",
    "\n",
    "print(f\"   Corrélations positives : {sum(edges_df['correlation'] > 0)}\")\n",
    "print(f\"   Corrélations négatives : {sum(edges_df['correlation'] < 0)}\")\n",
    "\n",
    "# positions circulaires des ROI (vue sagittale)\n",
    "radius = 1.0\n",
    "positions = {}\n",
    "for roi in roi_names:\n",
    "    ang = angle_by_roi.get(roi, 0.0) \n",
    "    positions[roi] = (radius * np.cos(ang), radius * np.sin(ang))\n",
    "\n",
    "\n",
    "# couleurs des noeuds par regions anatomique\n",
    "def assign_node_color(roi_name):\n",
    "    \"\"\"Attribue couleur selon région anatomique\"\"\"\n",
    "    if 'Ctx_Frontal' in roi_name or 'Frontal' in roi_name:\n",
    "        return '#e74c3c'  # Rouge - Cortex Frontal\n",
    "    elif 'Ctx_Parietal' in roi_name or 'Parietal' in roi_name:\n",
    "        return '#3498db'  # Bleu - Cortex Pariétal\n",
    "    elif 'Ctx_Temporal' in roi_name or 'Temporal' in roi_name:\n",
    "        return '#2ecc71'  # Vert - Cortex Temporal\n",
    "    elif 'Ctx_Occipital' in roi_name or 'Occipital' in roi_name:\n",
    "        return '#f39c12'  # Orange - Cortex Occipital\n",
    "    elif 'Ctx_Cingulate' in roi_name or 'Cingulate' in roi_name:\n",
    "        return '#9b59b6'  # Violet - Cortex Cingulaire\n",
    "    elif 'Hippocampus' in roi_name or 'Amygdala' in roi_name:\n",
    "        return '#e67e22'  # Orange foncé - Système limbique\n",
    "    elif 'Thalamus' in roi_name or 'Hypothalamus' in roi_name:\n",
    "        return '#1abc9c'  # Turquoise - Diencéphale\n",
    "    elif 'Cerebellum' in roi_name:\n",
    "        return '#34495e'  # Gris foncé - Cervelet\n",
    "    elif 'Caudate' in roi_name or 'Putamen' in roi_name:\n",
    "        return '#16a085'  # Vert foncé - Noyaux gris centraux\n",
    "    elif 'Olfactory' in roi_name:\n",
    "        return '#c0392b'  # Rouge foncé - Bulbes olfactifs\n",
    "    elif 'Ventricle' in roi_name or 'WM_' in roi_name or 'Brain_Stem' in roi_name:\n",
    "        return '#95a5a6'  # Gris clair - Substance blanche/ventricules\n",
    "    else:\n",
    "        return '#7f8c8d'  # Gris - Autre\n",
    "\n",
    "node_colors = [assign_node_color(roi) for roi in roi_names]\n",
    "\n",
    "# creation de la figure\n",
    "fig, ax = plt.subplots(figsize=figsize, subplot_kw=dict(aspect=\"equal\"))\n",
    "ax.set_xlim(-1.7, 1.7)\n",
    "ax.set_ylim(-1.7, 1.7)\n",
    "ax.axis('off')\n",
    "\n",
    "# colormap pour gradient des arêtes\n",
    "norm = Normalize(vmin=seuil_corr, vmax=1.0)\n",
    "cmap = cm.get_cmap('YlOrRd')  # Jaune (faible) et Rouge (fort)\n",
    "\n",
    "# tracer les aretes \n",
    "for idx, edge in edges_df.iterrows():\n",
    "    source = edge['source']\n",
    "    target = edge['target']\n",
    "    corr = edge['correlation']\n",
    "    abs_corr = edge['abs_corr']\n",
    "    \n",
    "    # positions\n",
    "    x1, y1 = positions[source]\n",
    "    x2, y2 = positions[target]\n",
    "    \n",
    "    # couleur selon force (gradient)\n",
    "    if corr > 0:\n",
    "        color = cmap(norm(abs_corr))  # jaune-rouge pour positives\n",
    "    else:\n",
    "        color = '#1f77b4'  # Bleu pour négatives\n",
    "    \n",
    "    # epaisseur selon force de corrélation + mise à l'échelle progressive\n",
    "    width_factor = (abs_corr - seuil_corr) / (1 - seuil_corr)\n",
    "    width = 0.8 + width_factor * (edge_width_scale - 0.8)\n",
    "    \n",
    "    # transparence selon force\n",
    "    alpha = 0.4 + width_factor * 0.5\n",
    "    \n",
    "    # tracer l'arête\n",
    "    ax.plot([x1, x2], [y1, y2], \n",
    "            color=color, \n",
    "            linewidth=width, \n",
    "            alpha=alpha, \n",
    "            zorder=1,\n",
    "            solid_capstyle='round')\n",
    "\n",
    "# tracet les noeuds\n",
    "for i, roi in enumerate(roi_names):\n",
    "    x, y = positions[roi]\n",
    "    \n",
    "    # cercle pour le nœud\n",
    "    circle = plt.Circle((x, y), node_radius, \n",
    "                       color=node_colors[i], \n",
    "                       ec='black', \n",
    "                       linewidth=2, \n",
    "                       zorder=3)\n",
    "    ax.add_patch(circle)\n",
    "    \n",
    "    # position et rotation du texte\n",
    "    angle_deg = np.degrees(angle_by_roi[roi])\n",
    "    \n",
    "    if -90 < angle_deg < 90:\n",
    "        # à droite du cercle\n",
    "        ha = 'left'\n",
    "        x_text = x + label_distance\n",
    "        rotation = angle_deg\n",
    "    else:\n",
    "        # à gauche du cercle\n",
    "        ha = 'right'\n",
    "        x_text = x - label_distance\n",
    "        rotation = angle_deg - 180\n",
    "    \n",
    "    # texte du label\n",
    "    ax.text(x_text, y, roi_label, \n",
    "            fontsize=font_size, \n",
    "            ha=ha, \n",
    "            va='center',\n",
    "            rotation=rotation,\n",
    "            rotation_mode='anchor',\n",
    "            zorder=4,\n",
    "            fontweight='bold')\n",
    "\n",
    "# colorbar pour le gradient\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax, fraction=0.025, pad=0.04, aspect=30)\n",
    "cbar.set_label('Force de corrélation (|ρ|)', rotation=270, labelpad=25, fontsize=13, fontweight='bold')\n",
    "cbar.ax.tick_params(labelsize=11)\n",
    "\n",
    "# légende des groupes anatomiques\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color='#e74c3c', label='Cortex Frontal'),\n",
    "    mpatches.Patch(color='#3498db', label='Cortex Pariétal'),\n",
    "    mpatches.Patch(color='#2ecc71', label='Cortex Temporal'),\n",
    "    mpatches.Patch(color='#f39c12', label='Cortex Occipital'),\n",
    "    mpatches.Patch(color='#9b59b6', label='Cortex Cingulaire'),\n",
    "    mpatches.Patch(color='#e67e22', label='Hippocampe/Amygdale'),\n",
    "    mpatches.Patch(color='#1abc9c', label='Thalamus/Hypothalamus'),\n",
    "    mpatches.Patch(color='#34495e', label='Cervelet'),\n",
    "    mpatches.Patch(color='#16a085', label='Noyaux gris centraux'),\n",
    "    mpatches.Patch(color='#95a5a6', label='Substance blanche/Ventricules'),\n",
    "]\n",
    "\n",
    "legend1 = ax.legend(handles=legend_patches, \n",
    "                   loc='upper left', \n",
    "                   bbox_to_anchor=(-0.02, 1.02),\n",
    "                   fontsize=10,\n",
    "                   framealpha=0.95,\n",
    "                   title='Régions anatomiques',\n",
    "                   title_fontsize=11,\n",
    "                   edgecolor='black',\n",
    "                   fancybox=True)\n",
    "\n",
    "# épaisseur des arêtes\n",
    "legend_lines = [\n",
    "    Line2D([0], [0], color='gray', linewidth=1, label=f'|ρ| ≈ {seuil_corr:.2f}'),\n",
    "    Line2D([0], [0], color='gray', linewidth=3, label=f'|ρ| ≈ {(1+seuil_corr)/2:.2f}'),\n",
    "    Line2D([0], [0], color='gray', linewidth=5, label=f'|ρ| ≈ 1.00'),\n",
    "]\n",
    "\n",
    "legend2 = ax.legend(handles=legend_lines,\n",
    "                   loc='lower left',\n",
    "                   bbox_to_anchor=(-0.02, -0.02),\n",
    "                   fontsize=10,\n",
    "                   framealpha=0.95,\n",
    "                   title='Épaisseur des arêtes',\n",
    "                   title_fontsize=11,\n",
    "                   edgecolor='black',\n",
    "                   fancybox=True)\n",
    "\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "plt.title(titre, fontsize=22, fontweight='bold', pad=30)\n",
    "\n",
    "# calcul des statistiques réseau\n",
    "degree_count = {}\n",
    "for roi in roi_names:\n",
    "    degree_count[roi] = sum((edges_df['source'] == roi) | (edges_df['target'] == roi))\n",
    "\n",
    "degree_mean = np.mean(list(degree_count.values()))\n",
    "degree_std = np.std(list(degree_count.values()))\n",
    "mean_corr = edges_df['abs_corr'].mean()\n",
    "\n",
    "# encadrer avec statistiques\n",
    "stats_text = f\"Connexions affichées : {n_connections}/{n_possible} ({100*n_connections/n_possible:.1f}%)\\n\"\n",
    "stats_text += f\"Degré moyen : {degree_mean:.1f} ± {degree_std:.1f} connexions/nœud\\n\"\n",
    "stats_text += f\"Force moyenne : |ρ| = {mean_corr:.3f}\"\n",
    "\n",
    "ax.text(0, -1.52, stats_text, \n",
    "        fontsize=12, \n",
    "        ha='center',\n",
    "        bbox=dict(boxstyle='round,pad=0.8', facecolor='wheat', alpha=0.5, edgecolor='black', linewidth=1.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# export\n",
    "output_file = fichier_corr.replace('.xlsx', f'_connectogram_thresh{seuil_corr:.2f}.png')\n",
    "plt.savefig(output_file, dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "\n",
    "print(f\"\\n Connectogramme sauvegardé : {output_file}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# statistiques réseau detaillées\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTIQUES DU RÉSEAU\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nSeuil de corrélation : |ρ| ≥ {seuil_corr}\")\n",
    "print(f\"Nombre total de connexions : {n_connections}/{n_possible} ({100*n_connections/n_possible:.1f}%)\")\n",
    "print(f\"Densité du réseau : {2*n_connections/(n_roi*(n_roi-1)):.3f}\")\n",
    "\n",
    "print(f\"\\nForce des connexions :\")\n",
    "print(f\"  Moyenne : |ρ| = {mean_corr:.3f}\")\n",
    "print(f\"  Médiane : |ρ| = {edges_df['abs_corr'].median():.3f}\")\n",
    "print(f\"  Min : |ρ| = {edges_df['abs_corr'].min():.3f}\")\n",
    "print(f\"  Max : |ρ| = {edges_df['abs_corr'].max():.3f}\")\n",
    "\n",
    "print(f\"\\nDegré des nœuds (connectivité) :\")\n",
    "print(f\"  Moyenne : {degree_mean:.1f} ± {degree_std:.1f}\")\n",
    "print(f\"  Médiane : {np.median(list(degree_count.values())):.0f}\")\n",
    "print(f\"  Min : {min(degree_count.values())}\")\n",
    "print(f\"  Max : {max(degree_count.values())}\")\n",
    "\n",
    "# Top 5 hubs (régions les plus connectées)\n",
    "top_hubs = sorted(degree_count.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"\\n TOP 5 HUBS (régions les plus connectées) :\")\n",
    "for rank, (roi, degree) in enumerate(top_hubs, 1):\n",
    "    print(f\"  {rank}. {roi:35s} : {degree:3d} connexions ({100*degree/(n_roi-1):.1f}%)\")\n",
    "\n",
    "# Bottom 5 (régions les moins connectées)\n",
    "bottom_5 = sorted(degree_count.items(), key=lambda x: x[1])[:5]\n",
    "print(\"\\n BOTTOM 5 (régions les moins connectées) :\")\n",
    "for rank, (roi, degree) in enumerate(bottom_5, 1):\n",
    "    print(f\"  {rank}. {roi:35s} : {degree:3d} connexions ({100*degree/(n_roi-1):.1f}%)\")\n",
    "\n",
    "# distribution des connexions positives/négatives\n",
    "n_positive = sum(edges_df['correlation'] > 0)\n",
    "n_negative = sum(edges_df['correlation'] < 0)\n",
    "print(f\"\\nDistribution des corrélations :\")\n",
    "print(f\"  Positives : {n_positive} ({100*n_positive/n_connections:.1f}%)\")\n",
    "print(f\"  Négatives : {n_negative} ({100*n_negative/n_connections:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2dc364-202c-485b-a393-eb55a8583195",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fisher-z : Identification des connexions différentielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc6df5-7436-4309-b803-97901c8d9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "plt.rcParams.update({'font.family': 'Helvetica', 'pdf.fonttype': 42})\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"Olfactory_Bulb\": \"Olfactory Bulb\",\n",
    "    \"WM_Cerebellum\": \"Cerebellar WM\",\n",
    "    \"Ventricle_III_IV\": \"III–IV Ventricles\",\n",
    "    \"Ventricle_Lateral\": \"Lateral Ventricles\",\n",
    "    \"Brain_Stem\": \"Brainstem\",\n",
    "    \"WM_Ctx\": \"Subcortical WM\",\n",
    "    \"Ctx_Parietal\": \"Parietal Cortex\",\n",
    "    \"Ctx_Occipital\": \"Occipital Cortex\",\n",
    "    \"tx_Temporal\": \"Temporal Cortex\",\n",
    "    \"Ctx_Frontal\": \"Frontal Cortex\",\n",
    "    \"Ctx_Cingulate\": \"Cingulate Cortex\",\n",
    "    \"Cerebellum\": \"Cerebellum\",\n",
    "    \"Amygdala\": \"Amygdala\",\n",
    "    \"Hippocampus\": \"Hippocampus\",\n",
    "    \"Thalamus_Hyppothalamus\": \"Thalamus – Hypothalamus\",\n",
    "    \"Caudate_Putamen\": \"Caudate – Putamen\",\n",
    "}\n",
    "\n",
    "roi_order_sagittal = [\n",
    "    \"Thalamus_Hyppothalamus\",\"Olfactory_Bulb\",\"Ctx_Frontal\",\"WM_Ctx\",\"tx_Temporal\",\"Caudate_Putamen\",\"Amygdala\",\n",
    "    \"Ctx_Parietal\",\"Hippocampus\",\"Ctx_Occipital\",\"Cerebellum\",\"WM_Cerebellum\",\n",
    "    \"Brain_Stem\",\"Ventricle_III_IV\",\"Ctx_Cingulate\",\"Ventricle_Lateral\",\n",
    "]\n",
    "\n",
    "def assign_node_color(roi_name):\n",
    "    \"\"\"Couleurs des nœuds (identiques aux autres script).\"\"\"\n",
    "    if ('Frontal' in roi_name or 'Parietal' in roi_name or 'Temporal' in roi_name or\n",
    "        'Occipital' in roi_name or 'Cingulate' in roi_name):\n",
    "        return '#3498db' \n",
    "    elif ('Hippocampus' in roi_name or 'Amygdala' in roi_name or\n",
    "          'Thalamus' in roi_name or 'Hypothalamus' in roi_name or\n",
    "          'Caudate' in roi_name or 'Putamen' in roi_name or\n",
    "          'Olfactory' in roi_name):\n",
    "        return '#27ae60' \n",
    "    elif 'Cerebellum' in roi_name:\n",
    "        return '#c0392b' \n",
    "    elif 'Brain_Stem' in roi_name or 'Brainstem' in roi_name:\n",
    "        return '#8b4513' \n",
    "    elif 'WM_Ctx' in roi_name or 'Ventricle' in roi_name:\n",
    "        return '#95a5a6'  \n",
    "    else:\n",
    "        return '#7f8c8d'  \n",
    "\n",
    "# parametres de comparaison \n",
    "fichier_groupe1 = \"CTRL_DTPA_corr_spearman.xlsx\"\n",
    "fichier_groupe2 = \"ARD_DTPA_corr_spearman.xlsx\"\n",
    "\n",
    "n1 = 44  # CTRL\n",
    "n2 = 36  # ARD\n",
    "\n",
    "alpha = 0.05          # seuil pour FDR\n",
    "seuil_diff_min = 0.15  # |Δρ| minimal à afficher\n",
    "\n",
    "titre = \"CTRL vs ARD - DTPA\"\n",
    "groupe1_nom = \"CTRL\"\n",
    "groupe2_nom = \"ARD\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARAISON STATISTIQUE INTER-GROUPES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Groupe 1 : {groupe1_nom} (n={n1})\")\n",
    "print(f\"Groupe 2 : {groupe2_nom} (n={n2})\")\n",
    "\n",
    "# chargement\n",
    "df_corr1 = pd.read_excel(fichier_groupe1, index_col=0)\n",
    "df_corr2 = pd.read_excel(fichier_groupe2, index_col=0)\n",
    "\n",
    "# sanitation\n",
    "assert df_corr1.shape == df_corr2.shape, \"Les matrices doivent avoir la même taille.\"\n",
    "roi_names = df_corr1.index.tolist()\n",
    "n_roi = len(roi_names)\n",
    "print(f\"\\n Matrices chargées : {n_roi} ROI\")\n",
    "\n",
    "# statistiques Fisher z\n",
    "def fisher_z(r):\n",
    "    r = np.clip(r, -0.9999, 0.9999) \n",
    "    return 0.5 * np.log((1 + r) / (1 - r))\n",
    "\n",
    "z_corr1 = fisher_z(df_corr1.values)\n",
    "z_corr2 = fisher_z(df_corr2.values)\n",
    "\n",
    "results = []\n",
    "se_diff = np.sqrt(1/(n1-3) + 1/(n2-3))  # erreur standard diff de z\n",
    "\n",
    "for i in range(n_roi):\n",
    "    for j in range(i+1, n_roi):\n",
    "        r1 = df_corr1.iloc[i, j]\n",
    "        r2 = df_corr2.iloc[i, j]\n",
    "        z1 = z_corr1[i, j]\n",
    "        z2 = z_corr2[i, j]\n",
    "        delta_z = z2 - z1\n",
    "        delta_r = r2 - r1\n",
    "        z_stat = delta_z / se_diff\n",
    "        p_value = 2 * (1 - norm.cdf(abs(z_stat)))  # bilatéral\n",
    "\n",
    "        results.append({\n",
    "            'ROI_1': roi_names[i],\n",
    "            'ROI_2': roi_names[j],\n",
    "            'r_groupe1': r1,\n",
    "            'r_groupe2': r2,\n",
    "            'delta_r': delta_r,\n",
    "            'z_stat': z_stat,\n",
    "            'p_value': p_value\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "n_tests = len(results_df)\n",
    "print(f\"   Nomber of tests : {n_tests}\")\n",
    "\n",
    "# correction FDR BH\n",
    "print(f\"\\n Correction FDR (α = {alpha})...\")\n",
    "rejected, p_corrected, _, _ = multipletests(results_df['p_value'], alpha=alpha, method='fdr_bh')\n",
    "results_df['p_corrected'] = p_corrected\n",
    "results_df['significant'] = rejected\n",
    "\n",
    "n_signif = int(rejected.sum())\n",
    "print(f\"   Tests significatifs : {n_signif}/{n_tests} ({100*n_signif/n_tests:.1f}%)\")\n",
    "\n",
    "# filtre de l'affichage : significatifs + amplitude minimale\n",
    "results_signif = results_df[(results_df['significant']) & (np.abs(results_df['delta_r']) >= seuil_diff_min)].copy()\n",
    "n_signif_display = len(results_signif)\n",
    "print(f\"   To display (p_FDR<{alpha} and |Δρ|≥{seuil_diff_min}) : {n_signif_display}\")\n",
    "\n",
    "if n_signif_display > 0:\n",
    "    print(f\"\\n Δρ mean : {results_signif['delta_r'].mean():.3f} | \"\n",
    "          f\"|Δρ|min : {results_signif['delta_r'].abs().min():.3f} | \"\n",
    "          f\"|Δρ|max : {results_signif['delta_r'].abs().max():.3f}\")\n",
    "    n_increase = int((results_signif['delta_r'] > 0).sum())\n",
    "    n_decrease = int((results_signif['delta_r'] < 0).sum())\n",
    "else:\n",
    "    n_increase = 0\n",
    "    n_decrease = 0\n",
    "\n",
    "# export stats\n",
    "output_stats = f\"comparison_{groupe1_nom}_vs_{groupe2_nom}_DTPA_stats.xlsx\"\n",
    "results_df.to_excel(output_stats, index=False)\n",
    "print(f\"\\n Statistiques sauvegardées : {output_stats}\")\n",
    "\n",
    "# visualisation \n",
    "if n_signif_display > 0:\n",
    "\n",
    "    #  paramètres visuels identiques au connectome intra-groupe\n",
    "    figsize = (16, 16)\n",
    "    node_radius = 0.035\n",
    "    edge_width_scale = 6\n",
    "    font_size = 14\n",
    "    label_distance = 0.10\n",
    "    radius = 1.0\n",
    "\n",
    "    # ordre sagittal + angles (départ à 180°, sens horaire)\n",
    "    ordered_roi = [r for r in roi_order_sagittal if r in roi_names] + \\\n",
    "                  [r for r in roi_names if r not in roi_order_sagittal]\n",
    "\n",
    "    start_angle_deg = 180.0\n",
    "    angles_ordered = np.linspace(\n",
    "        np.deg2rad(start_angle_deg),\n",
    "        np.deg2rad(start_angle_deg) - 2*np.pi,\n",
    "        len(ordered_roi),\n",
    "        endpoint=False\n",
    "    )\n",
    "    angle_by_roi = {roi: ang for roi, ang in zip(ordered_roi, angles_ordered)}\n",
    "\n",
    "    #  positions\n",
    "    positions = {roi: (radius*np.cos(angle_by_roi[roi]),\n",
    "                       radius*np.sin(angle_by_roi[roi]))\n",
    "                 for roi in roi_names}\n",
    "\n",
    "    #  couleurs des nœuds\n",
    "    node_colors = [assign_node_color(roi) for roi in roi_names]\n",
    "\n",
    "    #  figure\n",
    "    fig, ax = plt.subplots(figsize=figsize, subplot_kw=dict(aspect=\"equal\"))\n",
    "    ax.set_xlim(-1.7, 1.7)\n",
    "    ax.set_ylim(-1.7, 1.7)\n",
    "    ax.axis('off')\n",
    "\n",
    "    #  colormap \n",
    "    max_abs_diff = results_signif['delta_r'].abs().max()\n",
    "    if max_abs_diff == 0:\n",
    "        max_abs_diff = 1e-6\n",
    "    norm = TwoSlopeNorm(vmin=-max_abs_diff, vcenter=0, vmax=max_abs_diff)\n",
    "    cmap = plt.colormaps['coolwarm_r']\n",
    "\n",
    "    # tracer arêtes significatifs\n",
    "    for _, row in results_signif.iterrows():\n",
    "        s, t = row['ROI_1'], row['ROI_2']\n",
    "        delta_r = row['delta_r']\n",
    "        abs_delta = abs(delta_r)\n",
    "\n",
    "        x1, y1 = positions[s]\n",
    "        x2, y2 = positions[t]\n",
    "\n",
    "        color = cmap(norm(delta_r))\n",
    "        width_factor = abs_delta / max_abs_diff\n",
    "        width = 0.8 + width_factor * (edge_width_scale - 0.8)\n",
    "        alpha = 0.4 + width_factor * 0.5\n",
    "\n",
    "        ax.plot([x1, x2], [y1, y2],\n",
    "                color=color, linewidth=width, alpha=alpha,\n",
    "                zorder=1, solid_capstyle='round')\n",
    "\n",
    "    # nœuds + labels radiaux horizontaux\n",
    "    for i, roi in enumerate(roi_names):\n",
    "        x, y = positions[roi]\n",
    "        circle = plt.Circle((x, y), node_radius,\n",
    "                            color=node_colors[i], ec='black', linewidth=2, zorder=3)\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "        disp_label = LABEL_MAP.get(roi, roi)\n",
    "        angle_rad = angle_by_roi[roi]\n",
    "        angle_deg = np.degrees(angle_rad)\n",
    "        text_radius = radius + label_distance\n",
    "        x_text = text_radius * np.cos(angle_rad)\n",
    "        y_text = text_radius * np.sin(angle_rad)\n",
    "        ha = 'left' if (-90 < angle_deg <= 90) else 'right'\n",
    "\n",
    "        ax.text(x_text, y_text, disp_label,\n",
    "                fontsize=font_size, ha=ha, va='center',\n",
    "                rotation=0, zorder=4, fontweight='bold')\n",
    "\n",
    "    # colorbar\n",
    "    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, fraction=0.025, pad=0.04, aspect=30)\n",
    "    cbar.set_label(f'Δρ Difference ({groupe2_nom} - {groupe1_nom})',\n",
    "                   rotation=270, labelpad=25, fontsize=13, fontweight='bold')\n",
    "    cbar.ax.tick_params(labelsize=11)\n",
    "\n",
    "    # légende unique\n",
    "    legend_lines = [\n",
    "        Line2D([0], [0], color='#2166ac', linewidth=3,\n",
    "               label=f'{groupe2_nom} > {groupe1_nom} (Increase)'),\n",
    "        Line2D([0], [0], color='#b2182b', linewidth=3,\n",
    "               label=f'{groupe2_nom} < {groupe1_nom} (Decrease)'),\n",
    "    ]\n",
    "\n",
    "    ax.legend(handles=legend_lines,\n",
    "              loc='upper left', bbox_to_anchor=(-0.02, 1.02),\n",
    "              fontsize=11, framealpha=0.95,\n",
    "              edgecolor='black', fancybox=True)\n",
    "\n",
    "    # titre et stats\n",
    "    plt.title(titre + \"\\n(Significant differences (FDR < 0.05)\",\n",
    "              fontsize=22, fontweight='bold')\n",
    "\n",
    "    stats_text = (\n",
    "        f\"Different connections (displayed): {n_signif_display}/{n_tests} \"\n",
    "        f\"({100*n_signif_display/n_tests:.1f}%)\\n\"\n",
    "        f\"Increase : {n_increase} | Decrease : {n_decrease}\\n\"\n",
    "        f\"Mean |Δρ| : {results_signif['delta_r'].abs().mean():.3f}\"\n",
    "    )\n",
    "    ax.text(0, -1.52, stats_text, fontsize=12, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.8', facecolor='wheat',\n",
    "                      alpha=0.5, edgecolor='black', linewidth=1.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # export\n",
    "    output_fig = f\"comparison_{groupe1_nom}_vs_{groupe2_nom}_DTPA_connectogram.png\"\n",
    "    plt.savefig(output_fig, dpi=600, bbox_inches='tight',\n",
    "                facecolor='white', edgecolor='none')\n",
    "    print(f\"\\n Connectogramme sauvegardé : {output_fig}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"\\n Pas de connectogramme généré (aucune différence significative)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af761b08-4764-44f1-b434-a5ea49318317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
